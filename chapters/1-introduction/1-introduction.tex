\documentclass[../../thesis.tex]{subfiles}

\begin{document}

In this thesis we investigate possible improvements on the TimeVQVAE model presented by... We investigate how a "self supervised learning extension" of the tokenization affects the learned representations, and the effect on the prior learning. In particular we investigate if the learned representations are more informative, in the sense that they simultaneously enables high quality reconstruction, and improved the downstream classification accuracy. For the generative model we investigate if the learned representations enables faster convergence during training, and how the quality of the synthetic samples are affected. \\



\section{Motivation}
	- The role and importance of time series.
	- The need for models that capture complex structures for which traditional statistical models fail. ML4ITS. 
	- Why do people care about time series generation (TSG)?
		- Applications
	- Why is (unsupervised) representation learning for time series interesting?
	- 

\section{Overview/structure}
\begin{itemize}
	\item Main inspirations
	\cite{TimeVQVAE} 
	\item Collaboration with Erlend
	\item Structure of the thesis
\end{itemize}
\section{Research questions}
	- Does VQVAE learn good representations for classification?
	- Will self a supervised learning approach (BT, VIbCReg) enhance the representations learned from VQVAE for classification, and how does is affect the reconstruction? 
	- Will more expressive representations improve prior model learning?
	- How does SSL VQVAE compare when we train a powerful prior model on top of it (MaskGIT)? 



\section{AI Declaration}

- Ethical and environmental impact consideration with basis in UN sustainability goals



\end{document}
