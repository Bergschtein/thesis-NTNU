\documentclass[../../thesis.tex]{subfiles}

\begin{document}

In this thesis we investigate possible improvements on the TimeVQVAE model presented by... We investigate how a "self supervised learning extension" of the tokenization affects the learned representations, and the effect on the prior learning. In particular we investigate if the learned representations are more informative, in the sense that they simultaneously enables high quality reconstruction, and improved the downstream classification accuracy. For the generative model we investigate if the learned representations enables faster convergence during training, and how the quality of the synthetic samples are affected. \\


- Motivation
	- Why do people care about time series generation (TSG)?
	- Why is (unsupervised) representation learning for time series interesting?
	- 
- Research question
	- Does VQVAE learn good representations for classification?
	- Will self a supervised learning approach (BT, VIbCReg) enhance the representations learned from VQVAE for classification, and how does is affect the reconstruction? 
	- Will more expressive representations improve prior model learning?
	- How does SSL VQVAE compare when we train a powerful prior model on top of it (MaskGIT)? 
- Structure
- AI Declaration
- Ethical and environmental impact consideration with basis in UN sustainability goals
- 
\end{document}
